"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€æœ€åˆã®ç”»é¢èª­ã¿è¾¼ã¿æ™‚ã«ã®ã¿å®Ÿè¡Œã•ã‚Œã‚‹åˆæœŸåŒ–å‡¦ç†ãŒè¨˜è¿°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# PyTorchåˆæœŸåŒ–å•é¡Œã®äº‹å‰å¯¾å¿œï¼ˆå¼·åŒ–ç‰ˆï¼‰
############################################################
import os
import sys
import warnings

# PyTorchã®åˆæœŸåŒ–å•é¡Œå¯¾ç­–ï¼ˆå®Œå…¨ç‰ˆãƒ»Tritonç«¶åˆè§£æ±ºï¼‰
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['TORCH_CPP_LOG_LEVEL'] = 'ERROR'
os.environ['PYTORCH_JIT_LOG_LEVEL'] = 'ERROR'

# Tritonåå‰ç©ºé–“é‡è¤‡å•é¡Œã®æ ¹æœ¬è§£æ±º
os.environ['TRITON_DISABLE_COMPILER_WARNINGS'] = '1'
os.environ['TRITON_DISABLE_LINE_INFO'] = '1' 
os.environ['TRITON_CACHE_DIR'] = os.path.join(os.getcwd(), '.triton_cache')
os.environ['TRITON_DISABLE_OPTIMIZATION'] = '1'

# PyTorchã‚¯ãƒ©ã‚¹ç™»éŒ²ã‚¨ãƒ©ãƒ¼ã®å®Œå…¨å›é¿
os.environ['TORCH_LIBRARY_LAZY_INIT'] = 'TRUE'
os.environ['PYTORCH_DISABLE_PER_OP_PROFILING'] = '1'
os.environ['PYTORCH_DISABLE_CUDNN_INITIALIZATION'] = '1'
os.environ['TORCH_DISABLE_CUDA_INIT_CHECK'] = '1'

# PyTorchãƒ­ã‚°è¨­å®šã®å®Œå…¨ç„¡åŠ¹åŒ–
os.environ.pop('TORCH_LOGS', None)  # å•é¡Œã®ã‚ã‚‹ãƒ­ã‚°è¨­å®šã‚’å‰Šé™¤
os.environ['PYTORCH_DISABLE_INTERNAL_LOGGING'] = '1'

# PyTorchã®é…å»¶åˆæœŸåŒ–å¼·åˆ¶
os.environ['TORCH_LAZY_INIT'] = '1'

# NumPy 2.0äº’æ›æ€§ã®æœ€å„ªå…ˆå¯¾å¿œï¼ˆå…¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å‰ï¼‰
# ChromaDBã€LangChainã€ãã®ä»–ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒNumPyå±æ€§ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«å®Ÿè¡Œ
try:
    import numpy as np
    
    # NumPy 2.0ã§å‰Šé™¤ã•ã‚ŒãŸå±æ€§ã®å®Œå…¨å¾©å…ƒï¼ˆChromaDBäº’æ›æ€§ä¿è¨¼ï¼‰
    numpy_2_compatibility_attrs = {
        'float_': np.float64,
        'int_': np.int64,
        'complex_': np.complex128,
        'uint': np.uint32,
        'bool_': bool,
        # ChromaDBã§ä½¿ç”¨ã•ã‚Œã‚‹è¿½åŠ å±æ€§
        'int8': np.int8,
        'int16': np.int16,
        'int32': np.int32,
        'uint8': np.uint8,
        'uint16': np.uint16,
        'uint32': np.uint32,
        'uint64': np.uint64,
        'float16': np.float16,
        'float32': np.float32,
        'float64': np.float64,
        'complex64': np.complex64,
        'complex128': np.complex128
    }
    
    # å­˜åœ¨ã—ãªã„å±æ€§ã®ã¿ã‚’è¿½åŠ ï¼ˆæ—¢å­˜å±æ€§ã‚’ä¸Šæ›¸ãã—ãªã„ï¼‰
    for attr_name, attr_value in numpy_2_compatibility_attrs.items():
        if not hasattr(np, attr_name):
            setattr(np, attr_name, attr_value)
    
    print(f"NumPy {np.__version__} å®Œå…¨äº’æ›æ€§å¯¾å¿œå®Œäº†ï¼ˆChromaDBå¯¾å¿œï¼‰")
    
except ImportError:
    print("NumPyæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - äº’æ›æ€§å¯¾å¿œã‚’ã‚¹ã‚­ãƒƒãƒ—")
    pass

# å„ç¨®è­¦å‘Šã®å®Œå…¨æŠ‘åˆ¶ï¼ˆPyTorch/Tritonç‰¹åŒ–å¼·åŒ–ç‰ˆï¼‰
warnings.filterwarnings('ignore', category=UserWarning, module='torch')
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', message='.*triton.*')
warnings.filterwarnings('ignore', message='.*TORCH_LIBRARY.*')
warnings.filterwarnings('ignore', message='.*TORCH_LIBRARY_IMPL.*')
warnings.filterwarnings('ignore', message='.*TORCH_LIBRARY_FRAGMENT.*')
warnings.filterwarnings('ignore', message='.*namespace.*')
warnings.filterwarnings('ignore', message='.*float_.*')
warnings.filterwarnings('ignore', message='.*torch.classes.*')
warnings.filterwarnings('ignore', message='.*__path__._path.*')
warnings.filterwarnings('ignore', message='.*np.float_.*')
warnings.filterwarnings('ignore', message='.*Tried to instantiate class.*')
warnings.filterwarnings('ignore', category=UserWarning, module='triton')
warnings.filterwarnings('ignore', category=UserWarning, message='.*triton.*')
warnings.filterwarnings('ignore', category=RuntimeWarning, message='.*torch.*')

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ï¼ˆPyTorchäº’æ›æ€§å¼·åŒ–ç‰ˆï¼‰
############################################################
import logging
from logging.handlers import TimedRotatingFileHandler
from uuid import uuid4
import sys
import unicodedata
from datetime import datetime
from dotenv import load_dotenv
import streamlit as st
from docx import Document

# PyTorchãŠã‚ˆã³é–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å®‰å…¨ãªèª­ã¿è¾¼ã¿
try:
    # PyTorché–¢é€£ãƒ­ã‚°ã®äº‹å‰æŠ‘åˆ¶
    logging.getLogger('torch').setLevel(logging.CRITICAL)
    logging.getLogger('triton').setLevel(logging.CRITICAL)
    
    # PyTorchã®é…å»¶åˆæœŸåŒ–
    import torch
    torch.set_warn_always(False)
    if hasattr(torch, '_C'):
        torch._C._set_print_stacktraces_on_fatal_signal(False)
    
except Exception as torch_import_error:
    print(f"PyTorchèª­ã¿è¾¼ã¿è­¦å‘Šï¼ˆæ©Ÿèƒ½ã«ã¯å½±éŸ¿ãªã—ï¼‰: {torch_import_error}")

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import constants as ct


############################################################
# è¨­å®šé–¢é€£
############################################################
# ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã—ãŸç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def initialize():
    """
    ç”»é¢èª­ã¿è¾¼ã¿æ™‚ã«å®Ÿè¡Œã™ã‚‹åˆæœŸåŒ–å‡¦ç†
    NumPy 2.0ã¨PyTorchäº’æ›æ€§å•é¡Œã®å®Œå…¨å¯¾å¿œç‰ˆ
    """
    try:
        # ã€æœ€é‡è¦ã€‘äº’æ›æ€§ç¢ºä¿ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        setup_environment_variables()
        
        # åˆæœŸåŒ–ãƒ‡ãƒ¼ã‚¿ã®ç”¨æ„
        initialize_session_state()
        # ãƒ­ã‚°å‡ºåŠ›ç”¨ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’ç”Ÿæˆ
        initialize_session_id()
        # ãƒ­ã‚°å‡ºåŠ›ã®è¨­å®š
        initialize_logger()
        # RAGã®Retrieverã‚’ä½œæˆ
        initialize_retriever()
        
    except Exception as e:
        # åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›
        print(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {str(e)}")
        import traceback
        print(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«æœ€å°é™ã®åˆæœŸåŒ–
        if "messages" not in st.session_state:
            st.session_state.messages = []
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []
        if "retriever" not in st.session_state:
            st.session_state.retriever = None
        if "enhanced_mode" not in st.session_state:
            st.session_state.enhanced_mode = True  # é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«å¤‰æ›´
            
        # ã‚¨ãƒ©ãƒ¼ã‚’å†ç™ºç”Ÿã•ã›ã‚‹
        raise e


def setup_environment_variables():
    """
    ç’°å¢ƒå¤‰æ•°ã®äº‹å‰è¨­å®šã¨ã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–
    NumPy 2.0ã¨PyTorchäº’æ›æ€§å•é¡Œã®è§£æ±ºï¼ˆç°¡æ˜“ç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    """
    # USER_AGENTè¨­å®šï¼ˆæœªè¨­å®šã®å ´åˆã®ã¿ï¼‰
    if not os.environ.get('USER_AGENT'):
        os.environ['USER_AGENT'] = 'Enhanced-RAG-System/1.0.0'
    
    # PyTorché–¢é€£ã®å®‰å®šåŒ–è¨­å®š
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    os.environ['TORCH_LOGS'] = '+dynamo'
    
    # PyTorchåˆæœŸåŒ–å•é¡Œã®å›é¿
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['TORCH_CPP_LOG_LEVEL'] = 'ERROR'
    
    # NumPy 2.0é–¢é€£ã®è¨­å®š
    os.environ['NPY_PROMOTION_STATE'] = 'weak'
    
    # ChromaDBé–¢é€£ã®è¨­å®š
    os.environ['ALLOW_RESET'] = 'TRUE'
    
    # å®‰å…¨ãªè­¦å‘ŠæŠ‘åˆ¶ï¼ˆã‚¨ãƒ©ãƒ¼ã‚’å›é¿ï¼‰
    try:
        import warnings
        warnings.filterwarnings('ignore', category=UserWarning, module='torch')
        warnings.filterwarnings('ignore', category=FutureWarning)
        warnings.filterwarnings('ignore', category=DeprecationWarning, module='numpy')
        # messageãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã†å ´åˆã¯å®‰å…¨ã«å®Ÿè¡Œ
        try:
            warnings.filterwarnings('ignore', message='.*float_.*')
            warnings.filterwarnings('ignore', message='.*torch.classes.*')
        except (TypeError, AssertionError):
            pass  # messageãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå•é¡Œã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    except Exception:
        pass  # è­¦å‘Šè¨­å®šå…¨ä½“ãŒå•é¡Œã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    
    # NumPy 2.0äº’æ›æ€§ã®å®Œå…¨å¯¾å¿œï¼ˆChromaDBå¯¾å¿œå¼·åŒ–ç‰ˆãƒ»é‡è¤‡ç¢ºèªä»˜ãï¼‰
    try:
        import numpy as np
        
        # ChromaDBã®api/types.pyã§ä½¿ç”¨ã•ã‚Œã‚‹å…¨å±æ€§ã®å®Œå…¨å¾©å…ƒ
        critical_numpy_attrs = {
            # ChromaDB api/types.py ã§ç›´æ¥ä½¿ç”¨
            'float_': np.float64,
            'int_': np.int64,
            'uint': np.uint32,
            'complex_': np.complex128,
            'bool_': np.bool,
            
            # ãã®ä»–ã®äº’æ›æ€§å±æ€§
            'int8': np.int8, 'int16': np.int16, 'int32': np.int32,
            'uint8': np.uint8, 'uint16': np.uint16, 'uint64': np.uint64,
            'float16': np.float16, 'float32': np.float32, 'float64': np.float64,
            'complex64': np.complex64, 'complex128': np.complex128
        }
        
        # é‡è¦ï¼šå­˜åœ¨ç¢ºèªå¾Œã®å®‰å…¨ãªå±æ€§è¨­å®š
        missing_attrs = []
        for attr_name, attr_value in critical_numpy_attrs.items():
            if not hasattr(np, attr_name):
                try:
                    setattr(np, attr_name, attr_value)
                    missing_attrs.append(attr_name)
                except Exception as e:
                    print(f"NumPyå±æ€§è¨­å®šã‚¨ãƒ©ãƒ¼ {attr_name}: {e}")
        
        if missing_attrs:
            print(f"NumPy {np.__version__} äº’æ›æ€§å¯¾å¿œå®Œäº† - å¾©å…ƒå±æ€§: {', '.join(missing_attrs)}")
        else:
            print(f"NumPy {np.__version__} - å…¨äº’æ›æ€§å±æ€§ãŒæ—¢ã«å­˜åœ¨")
                
    except ImportError:
        print("NumPyæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - äº’æ›æ€§å¯¾å¿œã‚¹ã‚­ãƒƒãƒ—")
    except Exception as e:
        print(f"NumPyäº’æ›æ€§å¯¾å¿œã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œå¯èƒ½ï¼‰: {e}")
    
    # PyTorch/Tritonäº’æ›æ€§ã®è¿½åŠ å¯¾å¿œ
    try:
        import torch
        # PyTorchã®é™éŸ³åŒ–å¼·åŒ–
        torch.set_warn_always(False)
        if hasattr(torch, '_C') and hasattr(torch._C, '_set_print_stacktraces_on_fatal_signal'):
            torch._C._set_print_stacktraces_on_fatal_signal(False)
        
        # Tritonã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºä¿
        triton_cache_dir = os.path.join(os.getcwd(), '.triton_cache')
        os.makedirs(triton_cache_dir, exist_ok=True)
        
        # PyTorchãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®å®Œå…¨åˆ¶å¾¡
        logging.getLogger('torch').setLevel(logging.CRITICAL)
        
        # Tritonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å¯¾å¿œï¼ˆSEOç‰¹åŒ–ç‰ˆã§ã¯ä¸è¦ï¼‰
        # SEOç”¨é€”ã§ã¯Tritonã¯ä½¿ç”¨ã—ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
        
        print("PyTorchäº’æ›æ€§å¯¾å¿œå®Œäº†")
        
    except Exception as torch_error:
        print(f"PyTorchäº’æ›æ€§å¯¾å¿œï¼ˆè­¦å‘Šã®ã¿ï¼‰: {torch_error}")
        pass
    
    # æœ€çµ‚çš„ãªäº’æ›æ€§ç¢ºèª
    try:
        # ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ
        test_result = "ç’°å¢ƒè¨­å®šå®Œäº†: PyTorch/NumPyäº’æ›æ€§ç¢ºä¿"
        print(test_result)
        return True
        
    except Exception as final_error:
        print(f"æœ€çµ‚äº’æ›æ€§ç¢ºèªã§ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œå¯èƒ½ï¼‰: {final_error}")
        return False


def initialize_logger():
    """
    ãƒ­ã‚°å‡ºåŠ›ã®è¨­å®š
    """
    # æŒ‡å®šã®ãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€å­˜åœ¨ã—ãªã‘ã‚Œã°æ–°è¦ä½œæˆ
    os.makedirs(ct.LOG_DIR_PATH, exist_ok=True)
    
    # å¼•æ•°ã«æŒ‡å®šã—ãŸåå‰ã®ãƒ­ã‚¬ãƒ¼ï¼ˆãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã‚’å–å¾—
    # å†åº¦åˆ¥ã®ç®‡æ‰€ã§å‘¼ã³å‡ºã—ãŸå ´åˆã€ã™ã§ã«åŒã˜åå‰ã®ãƒ­ã‚¬ãƒ¼ãŒå­˜åœ¨ã—ã¦ã„ã‚Œã°èª­ã¿è¾¼ã‚€
    logger = logging.getLogger(ct.LOGGER_NAME)

    # ã™ã§ã«ãƒ­ã‚¬ãƒ¼ã«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆãƒ­ã‚°ã®å‡ºåŠ›å…ˆã‚’åˆ¶å¾¡ã™ã‚‹ã‚‚ã®ï¼‰ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€åŒã˜ãƒ­ã‚°å‡ºåŠ›ãŒè¤‡æ•°å›è¡Œã‚ã‚Œãªã„ã‚ˆã†å‡¦ç†ã‚’ä¸­æ–­ã™ã‚‹
    if logger.hasHandlers():
        return

    # 1æ—¥å˜ä½ã§ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‚’ãƒªã‚»ãƒƒãƒˆã—ã€åˆ‡ã‚Šæ›¿ãˆã‚‹è¨­å®š
    log_handler = TimedRotatingFileHandler(
        os.path.join(ct.LOG_DIR_PATH, ct.LOG_FILE),
        when="D",
        encoding="utf8"
    )
    # å‡ºåŠ›ã™ã‚‹ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®šç¾©
    # - ã€Œlevelnameã€: ãƒ­ã‚°ã®é‡è¦åº¦ï¼ˆINFO, WARNING, ERRORãªã©ï¼‰
    # - ã€Œasctimeã€: ãƒ­ã‚°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆã„ã¤è¨˜éŒ²ã•ã‚ŒãŸã‹ï¼‰
    # - ã€Œlinenoã€: ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œç•ªå·
    # - ã€ŒfuncNameã€: ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚ŒãŸé–¢æ•°å
    # - ã€Œsession_idã€: ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆèª°ã®ã‚¢ãƒ—ãƒªæ“ä½œã‹åˆ†ã‹ã‚‹ã‚ˆã†ã«ï¼‰
    # - ã€Œmessageã€: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    formatter = logging.Formatter(
        f"[%(levelname)s] %(asctime)s line %(lineno)s, in %(funcName)s, session_id={st.session_state.session_id}: %(message)s"
    )

    # å®šç¾©ã—ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã®é©ç”¨
    log_handler.setFormatter(formatter)

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ã€ŒINFOã€ã«è¨­å®š
    logger.setLevel(logging.INFO)

    # ä½œæˆã—ãŸãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆãƒ­ã‚°å‡ºåŠ›å…ˆã‚’åˆ¶å¾¡ã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã‚’ã€
    # ãƒ­ã‚¬ãƒ¼ï¼ˆãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Ÿéš›ã«ç”Ÿæˆã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã«è¿½åŠ ã—ã¦ãƒ­ã‚°å‡ºåŠ›ã®æœ€çµ‚è¨­å®š
    logger.addHandler(log_handler)


def initialize_session_id():
    """
    ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®ä½œæˆ
    """
    if "session_id" not in st.session_state:
        # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡å­—åˆ—ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼‰ã‚’ã€ãƒ­ã‚°å‡ºåŠ›ç”¨ã«ä½œæˆ
        st.session_state.session_id = uuid4().hex


def initialize_session_state():
    """
    åˆæœŸåŒ–ãƒ‡ãƒ¼ã‚¿ã®ç”¨æ„
    """
    if "messages" not in st.session_state:
        # ã€Œè¡¨ç¤ºç”¨ã€ã®ä¼šè©±ãƒ­ã‚°ã‚’é †æ¬¡æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã‚’ç”¨æ„
        st.session_state.messages = []
        # ã€ŒLLMã¨ã®ã‚„ã‚Šã¨ã‚Šç”¨ã€ã®ä¼šè©±ãƒ­ã‚°ã‚’é †æ¬¡æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã‚’ç”¨æ„
        st.session_state.chat_history = []


def create_persistent_vector_store_safely(splitted_docs, embeddings, logger, batch_size=50, persist_directory=None):
    """
    æ°¸ç¶šåŒ–å¯¾å¿œã®å®‰å…¨ãªãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆï¼ˆãƒãƒƒãƒå‡¦ç†ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¯¾å¿œï¼‰
    """
    try:
        # æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
        if persist_directory:
            os.makedirs(persist_directory, exist_ok=True)
            logger.info(f"æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: {persist_directory}")
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å‹•çš„èª¿æ•´
        if len(splitted_docs) > 100:
            adjusted_batch_size = min(batch_size, 25)
            logger.info(f"å¤§é‡æ–‡æ›¸æ¤œçŸ¥ï¼šãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’{adjusted_batch_size}ã«èª¿æ•´")
        else:
            adjusted_batch_size = batch_size
        
        # Step 1: ãƒ†ã‚¹ãƒˆç”¨å°ãƒãƒƒãƒã§ã®å‹•ä½œç¢ºèª
        test_docs = splitted_docs[:min(3, len(splitted_docs))]
        if persist_directory:
            test_db = Chroma.from_documents(
                documents=test_docs,
                embedding=embeddings,
                persist_directory=persist_directory + "_test"
            )
        else:
            test_db = Chroma.from_documents(
                documents=test_docs,
                embedding=embeddings
            )
        logger.info("ãƒ†ã‚¹ãƒˆç”¨ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆæˆåŠŸ")
        
        # Step 2: å…¨æ–‡æ›¸ã§ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
        if len(splitted_docs) <= adjusted_batch_size:
            # å°è¦æ¨¡ï¼šä¸€æ‹¬å‡¦ç†
            if persist_directory:
                db = Chroma.from_documents(
                    documents=splitted_docs,
                    embedding=embeddings,
                    persist_directory=persist_directory
                )
            else:
                db = Chroma.from_documents(
                    documents=splitted_docs,
                    embedding=embeddings
                )
            logger.info(f"å°è¦æ¨¡ä¸€æ‹¬å‡¦ç†å®Œäº†ï¼š{len(splitted_docs)}æ–‡æ›¸")
        else:
            # å¤§è¦æ¨¡ï¼šãƒãƒƒãƒå‡¦ç†
            logger.info(f"å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç†é–‹å§‹ï¼š{len(splitted_docs)}æ–‡æ›¸ã€ãƒãƒƒãƒã‚µã‚¤ã‚º{adjusted_batch_size}")
            
            # æœ€åˆã®ãƒãƒƒãƒã§ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢åˆæœŸåŒ–
            initial_batch = splitted_docs[:adjusted_batch_size]
            if persist_directory:
                db = Chroma.from_documents(
                    documents=initial_batch,
                    embedding=embeddings,
                    persist_directory=persist_directory
                )
            else:
                db = Chroma.from_documents(
                    documents=initial_batch,
                    embedding=embeddings
                )
            
            # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’é †æ¬¡è¿½åŠ 
            for i in range(adjusted_batch_size, len(splitted_docs), adjusted_batch_size):
                batch = splitted_docs[i:i + adjusted_batch_size]
                try:
                    db.add_documents(batch)
                    logger.info(f"ãƒãƒƒãƒ {i//adjusted_batch_size + 1} å®Œäº†ï¼š{len(batch)}æ–‡æ›¸è¿½åŠ ")
                except Exception as batch_error:
                    logger.warning(f"ãƒãƒƒãƒ {i//adjusted_batch_size + 1} ã§ã‚¨ãƒ©ãƒ¼ã€ã‚¹ã‚­ãƒƒãƒ—: {batch_error}")
                    continue
            
            logger.info("å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç†å®Œäº†")
        
        # æ°¸ç¶šåŒ–ã®å ´åˆã¯ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¨˜éŒ²
        if persist_directory:
            try:
                version_file = os.path.join(os.path.dirname(persist_directory), "db_version.txt")
                with open(version_file, 'w', encoding='utf-8') as f:
                    f.write(f"{ct.CURRENT_DB_VERSION}\n")
                    f.write(f"created_at: {datetime.now().isoformat()}\n")
                    f.write(f"documents_count: {len(splitted_docs)}\n")
                logger.info(f"DBãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±è¨˜éŒ²å®Œäº†: {version_file}")
            except Exception as version_error:
                logger.warning(f"ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œå¯èƒ½ï¼‰: {version_error}")
        
        return db
        
    except Exception as e:
        logger.error(f"æ°¸ç¶šåŒ–ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šéæ°¸ç¶šåŒ–ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢
        logger.warning("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šéæ°¸ç¶šåŒ–ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã§åˆæœŸåŒ–")
        if persist_directory:
            return Chroma(embedding_function=embeddings)
        else:
            return Chroma(embedding_function=embeddings)


def create_vector_store_safely(splitted_docs, embeddings, logger, batch_size=50):
    """
    å¾“æ¥ã®éæ°¸ç¶šåŒ–ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
    """
    return create_persistent_vector_store_safely(splitted_docs, embeddings, logger, batch_size, None)

def check_persistent_db_exists():
    """
    æ°¸ç¶šåŒ–DBã®å­˜åœ¨ç¢ºèªã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
    
    Returns:
        bool: æœ‰åŠ¹ãªæ°¸ç¶šåŒ–DBãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹
        str: å­˜åœ¨ç¢ºèªã®è©³ç´°ç†ç”±
    """
    try:
        # DBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(ct.PERSISTENT_DB_PATH):
            return False, "æ°¸ç¶šåŒ–DBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“"
        
        # ChromaãŒå¿…è¦ã¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        required_files = ['chroma.sqlite3']  # Chromaã®åŸºæœ¬ãƒ•ã‚¡ã‚¤ãƒ«
        for required_file in required_files:
            file_path = os.path.join(ct.PERSISTENT_DB_PATH, required_file)
            if not os.path.exists(file_path):
                return False, f"å¿…è¦ãªDBãƒ•ã‚¡ã‚¤ãƒ« {required_file} ãŒå­˜åœ¨ã—ã¾ã›ã‚“"
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã®ç¢ºèª
        version_file = os.path.join(os.path.dirname(ct.PERSISTENT_DB_PATH), "db_version.txt")
        if os.path.exists(version_file):
            try:
                with open(version_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if ct.CURRENT_DB_VERSION in content:
                        return True, f"æœ‰åŠ¹ãªæ°¸ç¶šåŒ–DBç™ºè¦‹ (ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {ct.CURRENT_DB_VERSION})"
                    else:
                        return False, f"DBãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒå¤ã„ãŸã‚å†æ§‹ç¯‰ãŒå¿…è¦ã§ã™"
            except Exception as version_error:
                return False, f"ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {version_error}"
        else:
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯å†æ§‹ç¯‰
            return False, "ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ãŒãªã„ãŸã‚å®‰å…¨ã®ãŸã‚å†æ§‹ç¯‰ã—ã¾ã™"
            
    except Exception as e:
        return False, f"æ°¸ç¶šåŒ–DBç¢ºèªã‚¨ãƒ©ãƒ¼: {e}"


def load_persistent_db(embeddings, logger):
    """
    æ—¢å­˜ã®æ°¸ç¶šåŒ–DBã‚’ãƒ­ãƒ¼ãƒ‰
    
    Args:
        embeddings: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
        logger: ãƒ­ã‚¬ãƒ¼
        
    Returns:
        Chroma: ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢
    """
    try:
        logger.info(f"æ°¸ç¶šåŒ–DBãƒ­ãƒ¼ãƒ‰é–‹å§‹: {ct.PERSISTENT_DB_PATH}")
        
        # æ°¸ç¶šåŒ–DBã®ãƒ­ãƒ¼ãƒ‰
        db = Chroma(
            persist_directory=ct.PERSISTENT_DB_PATH,
            embedding_function=embeddings
        )
        
        # ãƒ­ãƒ¼ãƒ‰æˆåŠŸç¢ºèªï¼ˆç°¡å˜ãªãƒ†ã‚¹ãƒˆæ¤œç´¢ï¼‰
        test_results = db.similarity_search("SEO", k=1)
        if test_results:
            logger.info(f"æ°¸ç¶šåŒ–DBãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {len(test_results)}ä»¶ã®ãƒ†ã‚¹ãƒˆæ¤œç´¢çµæœ")
            return db
        else:
            logger.warning("æ°¸ç¶šåŒ–DBã¯ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã—ãŸãŒã€æ¤œç´¢çµæœãŒç©ºã§ã™")
            return db
            
    except Exception as e:
        logger.error(f"æ°¸ç¶šåŒ–DBãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        raise e


def initialize_retriever():
    """
    ç”»é¢èª­ã¿è¾¼ã¿æ™‚ã«RAGã®Retrieverï¼ˆãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰æ¤œç´¢ã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã‚’ä½œæˆ
    æ°¸ç¶šåŒ–å¯¾å¿œç‰ˆï¼ˆé«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ  + æ°¸ç¶šåŒ–DBï¼‰
    """
    # ãƒ­ã‚¬ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã§ã€å¾Œç¶šã®å‡¦ç†ä¸­ã«ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼ãªã©ãŒãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã•ã‚Œã‚‹
    logger = logging.getLogger(ct.LOGGER_NAME)

    # ã™ã§ã«RetrieverãŒä½œæˆæ¸ˆã¿ã®å ´åˆã€å¾Œç¶šã®å‡¦ç†ã‚’ä¸­æ–­
    if "retriever" in st.session_state:
        return
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ç”¨æ„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œç‰ˆï¼‰
    embeddings = OpenAIEmbeddings(
        chunk_size=1000,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åˆ¶é™
        max_retries=2,    # ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’åˆ¶é™
        request_timeout=60  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
    )
    
    # Step 1: æ°¸ç¶šåŒ–DBã®å­˜åœ¨ç¢ºèª
    db_exists, db_status_reason = check_persistent_db_exists()
    logger.info(f"æ°¸ç¶šåŒ–DBç¢ºèªçµæœ: {db_status_reason}")
    
    # ğŸš€ Streamlit Cloud ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã®å¼·åˆ¶å†æ§‹ç¯‰å¯¾å¿œ
    # ç’°å¢ƒå¤‰æ•° FORCE_REBUILD_DB=true ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€æ—¢å­˜DBã‚’ç„¡è¦–ã—ã¦å†æ§‹ç¯‰
    import os
    FORCE_REBUILD = os.getenv("FORCE_REBUILD_DB", "false").lower() == "true"
    if FORCE_REBUILD:
        logger.warning("âš ï¸ ç’°å¢ƒå¤‰æ•° FORCE_REBUILD_DB=true ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€VectorDBã‚’å¼·åˆ¶å†æ§‹ç¯‰ã—ã¾ã™")
        db_exists = False
    
    if db_exists:
        # æ—¢å­˜ã®æ°¸ç¶šåŒ–DBã‚’ãƒ­ãƒ¼ãƒ‰
        try:
            db = load_persistent_db(embeddings, logger)
            logger.info("æ—¢å­˜æ°¸ç¶šåŒ–DBã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸ")
            
            # é«˜ç²¾åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šã¯å¾Œã§è¡Œã†
            # ã¾ãšã¯DBãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã§ãã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
            
        except Exception as load_error:
            logger.error(f"æ°¸ç¶šåŒ–DBãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€æ–°è¦ä½œæˆã—ã¾ã™: {load_error}")
            db_exists = False  # æ–°è¦ä½œæˆãƒ•ãƒ©ã‚°
    
    if not db_exists:
        # æ–°è¦æ°¸ç¶šåŒ–DBã®ä½œæˆ
        logger.info("æ–°è¦æ°¸ç¶šåŒ–DBã‚’ä½œæˆã—ã¾ã™")
        
        # RAGã®å‚ç…§å…ˆã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
        docs_all = load_data_sources()

        # OSãŒWindowsã®å ´åˆã€Unicodeæ­£è¦åŒ–ã¨ã€cp932ï¼ˆWindowsç”¨ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼‰ã§è¡¨ç¾ã§ããªã„æ–‡å­—ã‚’é™¤å»
        for doc in docs_all:
            doc.page_content = adjust_string(doc.page_content)
            for key in doc.metadata:
                doc.metadata[key] = adjust_string(doc.metadata[key])
    
        # æ–‡æ›¸é‡ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®äº‹å‰ãƒã‚§ãƒƒã‚¯
        total_chars = sum(len(doc.page_content) for doc in docs_all)
        estimated_tokens = total_chars // 4  # æ¦‚ç®—ï¼š4æ–‡å­—=1ãƒˆãƒ¼ã‚¯ãƒ³
        logger.info(f"å‡¦ç†å¯¾è±¡æ–‡æ›¸: {len(docs_all)}ä»¶, åˆè¨ˆæ–‡å­—æ•°: {total_chars:,}, æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°: {estimated_tokens:,}")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«åŸºã¥ãå‹•çš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºèª¿æ•´
        if total_chars > ct.MAX_CHARS_BEFORE_SPLITTING:
            target_chunk_size = ct.LARGE_DATA_CHUNK_SIZE
            batch_size = min(ct.EMBEDDING_BATCH_SIZE // 2, 25)  # ã‚ˆã‚Šå°ã•ãªãƒãƒƒãƒ
            logger.warning(f"å¤§é‡æ–‡æ›¸æ¤œçŸ¥ï¼šãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’{target_chunk_size}ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’{batch_size}ã«ç¸®å°")
        else:
            target_chunk_size = ct.DEFAULT_CHUNK_SIZE
            batch_size = ct.EMBEDDING_BATCH_SIZE
            logger.info(f"æ¨™æº–å‡¦ç†ï¼šãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º{target_chunk_size}ã€ãƒãƒƒãƒã‚µã‚¤ã‚º{batch_size}")
        
        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šã®èª¿æ•´
        embeddings.chunk_size = batch_size
        
        # æ¨™æº–ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰
        logger.info("æ¨™æº–ãƒãƒ£ãƒ³ã‚¯ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨")
        text_splitter = CharacterTextSplitter(
            chunk_size=target_chunk_size,
            chunk_overlap=200,  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å¢—ã‚„ã—ã¦æ–‡è„ˆä¿æŒ
            separator="\n\n",  # ã‚ˆã‚Šé©åˆ‡ãªã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿
            length_function=len,
            is_separator_regex=False
        )
        splitted_docs = text_splitter.split_documents(docs_all)
        
        # åˆ†å‰²å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°å†ç¢ºèª
        chunk_chars = sum(len(doc.page_content) for doc in splitted_docs)
        chunk_tokens = chunk_chars // 4
        logger.info(f"åˆ†å‰²çµæœ: {len(splitted_docs)}ãƒãƒ£ãƒ³ã‚¯, æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°: {chunk_tokens:,}")

        # æ°¸ç¶šåŒ–ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆ
        db = create_persistent_vector_store_safely(
            splitted_docs, 
            embeddings, 
            logger, 
            batch_size, 
            ct.PERSISTENT_DB_PATH
        )
        logger.info("æ–°è¦æ°¸ç¶šåŒ–DBã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ")

    # å…±é€šï¼šæ±ç”¨çš„é«˜ç²¾åº¦RAGã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ï¼ˆChromaDBäº’æ›æ€§å¯¾å¿œï¼‰
    try:
        # æ®µéšçš„é«˜ç²¾åº¦åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        logger.info("æ±ç”¨é«˜ç²¾åº¦RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
        
        # ChromaDBäº’æ›æ€§ã‚’è€ƒæ…®ã—ãŸretrieverã®ä½œæˆï¼ˆé«˜ç²¾åº¦åŒ–ï¼‰
        # search_typeã¨search_kwargsã®é©åˆ‡ãªçµ„ã¿åˆã‚ã›ã‚’ä½¿ç”¨
        enhanced_retriever = db.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": 12,  # æ¤œç´¢çµæœæ•°ã‚’å¢—åŠ ï¼ˆé«˜ç²¾åº¦åŒ–ï¼‰
            }
        )
        
        # Step 2: é«˜ç²¾åº¦BM25ã¨ã®çµ„ã¿åˆã‚ã›ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        try:
            from langchain_community.retrievers import BM25Retriever
            # EnsembleRetrieverã¯æ–°ã—ã„LangChainãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯å»ƒæ­¢
            # BM25Retrieverã®ã¿ã‚’ä½¿ç”¨ã—ãŸä»£æ›¿å®Ÿè£…
            
            # BM25ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã®ä½œæˆï¼ˆDBã‹ã‚‰æ–‡æ›¸ã‚’å–å¾—ï¼‰
            # DBã‹ã‚‰å…¨æ–‡æ›¸ã‚’å–å¾—ã—ã¦BM25ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
            try:
                # DBã‹ã‚‰ã™ã¹ã¦ã®æ–‡æ›¸ã‚’å–å¾—
                all_docs = db.get()
                if all_docs and 'documents' in all_docs:
                    texts = all_docs['documents']
                    metadatas = all_docs.get('metadatas', [{}] * len(texts))
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç©ºã®ãƒªã‚¹ãƒˆã§åˆæœŸåŒ–
                    texts = ["ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆSEOæ–‡æ›¸"]
                    metadatas = [{"source": "default"}]
            except Exception as get_docs_error:
                logger.warning(f"DBæ–‡æ›¸å–å¾—ã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ–‡æ›¸ã§åˆæœŸåŒ–: {get_docs_error}")
                texts = ["ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆSEOæ–‡æ›¸"]
                metadatas = [{"source": "default"}]
            
            bm25_retriever = BM25Retriever.from_texts(
                texts, 
                metadatas=metadatas
            )
            bm25_retriever.k = 8  # BM25ã®æ¤œç´¢æ•°ã‚‚å¢—åŠ 
            
            # ä»£æ›¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿè£…ï¼ˆEnsembleRetrieverå»ƒæ­¢å¯¾å¿œï¼‰
            # ã‚«ã‚¹ã‚¿ãƒ ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã§ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã¨BM25ã‚’çµ„ã¿åˆã‚ã›
            class CustomEnsembleRetriever:
                def __init__(self, vector_retriever, bm25_retriever, vector_weight=0.7):
                    self.vector_retriever = vector_retriever
                    self.bm25_retriever = bm25_retriever
                    self.vector_weight = vector_weight
                    self.bm25_weight = 1.0 - vector_weight
                
                def get_relevant_documents(self, query, k=4):
                    return self._retrieve_documents(query, k)
                
                def invoke(self, query_input, k=4):
                    """æ–°ã—ã„LangChain APIå¯¾å¿œï¼ˆæ–‡å­—åˆ—ãƒ»è¾æ›¸ä¸¡å¯¾å¿œï¼‰"""
                    if isinstance(query_input, str):
                        query = query_input
                    elif isinstance(query_input, dict):
                        query = query_input.get("query", query_input.get("input", ""))
                    else:
                        query = str(query_input)
                    return self._retrieve_documents(query, k)
                
                def _retrieve_documents(self, query, k=8):  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¤œç´¢æ•°ã‚’8ã«å¢—åŠ 
                    """å®Ÿéš›ã®æ–‡æ›¸æ¤œç´¢å‡¦ç†ï¼ˆé«˜ç²¾åº¦åŒ–ï¼‰"""
                    # ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢çµæœï¼ˆæ¤œç´¢æ•°å¢—åŠ ï¼‰
                    try:
                        if hasattr(self.vector_retriever, 'invoke'):
                            vector_docs = self.vector_retriever.invoke(query)[:int(k*self.vector_weight)+2]
                        else:
                            vector_docs = self.vector_retriever.get_relevant_documents(query)[:int(k*self.vector_weight)+2]
                    except Exception as vector_error:
                        print(f"ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {vector_error}")
                        vector_docs = []
                    
                    # BM25æ¤œç´¢çµæœï¼ˆæ¤œç´¢æ•°å¢—åŠ ï¼‰
                    try:
                        if hasattr(self.bm25_retriever, 'invoke'):
                            bm25_docs = self.bm25_retriever.invoke(query)[:int(k*self.bm25_weight)+2]
                        else:
                            bm25_docs = self.bm25_retriever.get_relevant_documents(query)[:int(k*self.bm25_weight)+2]
                    except Exception as bm25_error:
                        print(f"BM25æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {bm25_error}")
                        bm25_docs = []
                    
                    # é‡è¤‡é™¤å»ã—ãªãŒã‚‰çµåˆ
                    combined_docs = []
                    seen_contents = set()
                    
                    for doc in vector_docs + bm25_docs:
                        if doc.page_content not in seen_contents:
                            combined_docs.append(doc)
                            seen_contents.add(doc.page_content)
                        if len(combined_docs) >= k:
                            break
                    
                    return combined_docs[:k]
            
            custom_retriever = CustomEnsembleRetriever(enhanced_retriever, bm25_retriever)
            
            st.session_state.retriever = custom_retriever
            st.session_state.enhanced_mode = True
            st.session_state._enhanced_type = "custom_ensemble"
            logger.info("ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é«˜ç²¾åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆãƒ™ã‚¯ã‚¿ãƒ¼+BM25ï¼‰")
            
        except (ImportError, Exception) as bm25_error:
            # BM25ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯å¼·åŒ–ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã®ã¿
            logger.warning(f"BM25åˆ©ç”¨ä¸å¯ã€ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢å¼·åŒ–ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š: {bm25_error}")
            
            # MMRï¼ˆMaximum Marginal Relevanceï¼‰æ¤œç´¢ã§å¤šæ§˜æ€§ã‚’ç¢ºä¿
            try:
                mmr_retriever = db.as_retriever(
                    search_type="mmr",
                    search_kwargs={
                        "k": 10,  # MMRæ¤œç´¢æ•°å¢—åŠ 
                        "lambda_mult": 0.6,  # å¤šæ§˜æ€§ã‚’è‹¥å¹²å‘ä¸Š
                        "fetch_k": 20,  # å€™è£œæ–‡æ›¸æ•°å¢—åŠ 
                    }
                )
                st.session_state.retriever = mmr_retriever
                st.session_state.enhanced_mode = True
                st.session_state._enhanced_type = "mmr_enhanced"
                logger.info("MMRå¼·åŒ–ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆé«˜ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            except Exception:
                # æœ€å¾Œã®æ‰‹æ®µï¼šæ¨™æº–é¡ä¼¼åº¦æ¤œç´¢
                st.session_state.retriever = enhanced_retriever
                st.session_state.enhanced_mode = True
                st.session_state._enhanced_type = "vector_enhanced"
                logger.info("æ¨™æº–ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆåŸºæœ¬é«˜ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
    except Exception as e:
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ¨™æº–ãƒ¢ãƒ¼ãƒ‰ï¼ˆé«˜å“è³ªè¨­å®šã‚’ç¶­æŒï¼‰
        logger.warning(f"é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—ã€æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š: {e}")
        st.session_state.retriever = db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        )
        st.session_state.enhanced_mode = True  # é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ã‚’ç¶­æŒ
        st.session_state._enhanced_type = "standard"


def initialize_session_state():
    """
    åˆæœŸåŒ–ãƒ‡ãƒ¼ã‚¿ã®ç”¨æ„
    """
    if "messages" not in st.session_state:
        # ã€Œè¡¨ç¤ºç”¨ã€ã®ä¼šè©±ãƒ­ã‚°ã‚’é †æ¬¡æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã‚’ç”¨æ„
        st.session_state.messages = []
        # ã€ŒLLMã¨ã®ã‚„ã‚Šã¨ã‚Šç”¨ã€ã®ä¼šè©±ãƒ­ã‚°ã‚’é †æ¬¡æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã‚’ç”¨æ„
        st.session_state.chat_history = []


def load_data_sources():
    """
    RAGã®å‚ç…§å…ˆã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿

    Returns:
        èª­ã¿è¾¼ã‚“ã é€šå¸¸ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
    """
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æ ¼ç´ã™ã‚‹ç”¨ã®ãƒªã‚¹ãƒˆ
    docs_all = []
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã®å®Ÿè¡Œï¼ˆæ¸¡ã—ãŸå„ãƒªã‚¹ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚Œã‚‹ï¼‰
    recursive_file_check(ct.RAG_TOP_FOLDER_PATH, docs_all)

    web_docs_all = []
    # ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã¯åˆ¥ã«ã€æŒ‡å®šã®Webãƒšãƒ¼ã‚¸å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿
    # èª­ã¿è¾¼ã¿å¯¾è±¡ã®Webãƒšãƒ¼ã‚¸ä¸€è¦§ã«å¯¾ã—ã¦å‡¦ç†
    for web_url in ct.WEB_URL_LOAD_TARGETS:
        # æŒ‡å®šã®Webãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿
        loader = WebBaseLoader(web_url)
        web_docs = loader.load()
        # foræ–‡ã®å¤–ã®ãƒªã‚¹ãƒˆã«èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’è¿½åŠ 
        web_docs_all.extend(web_docs)
    # é€šå¸¸èª­ã¿è¾¼ã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«Webãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    docs_all.extend(web_docs_all)

    return docs_all


def recursive_file_check(path, docs_all):
    """
    RAGã®å‚ç…§å…ˆã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿

    Args:
        path: èª­ã¿è¾¼ã¿å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        docs_all: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æ ¼ç´ã™ã‚‹ç”¨ã®ãƒªã‚¹ãƒˆ
    """
    # ãƒ‘ã‚¹ãŒãƒ•ã‚©ãƒ«ãƒ€ã‹ã©ã†ã‹ã‚’ç¢ºèª
    if os.path.isdir(path):
        # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆã€ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€åã®ä¸€è¦§ã‚’å–å¾—
        files = os.listdir(path)
        # å„ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦å‡¦ç†
        for file in files:
            # ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€åã ã‘ã§ãªãã€ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
            full_path = os.path.join(path, file)
            # ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æ¸¡ã—ã€å†å¸°çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã®é–¢æ•°ã‚’å®Ÿè¡Œ
            recursive_file_check(full_path, docs_all)
    else:
        # ãƒ‘ã‚¹ãŒãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        file_load(path, docs_all)


def file_load(path, docs_all):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿

    Args:
        path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        docs_all: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æ ¼ç´ã™ã‚‹ç”¨ã®ãƒªã‚¹ãƒˆ
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’å–å¾—
    file_extension = os.path.splitext(path)[1]
    # ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ã‚’å«ã‚€ï¼‰ã‚’å–å¾—
    file_name = os.path.basename(path)

    # æƒ³å®šã—ã¦ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®å ´åˆã®ã¿èª­ã¿è¾¼ã‚€
    if file_extension in ct.SUPPORTED_EXTENSIONS:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã«åˆã£ãŸdata loaderã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        loader_func = ct.SUPPORTED_EXTENSIONS[file_extension]
        
        # å¼·åŒ–docxãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆé–¢æ•°ï¼‰ã‹å¾“æ¥ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆã‚¯ãƒ©ã‚¹ï¼‰ã‹ã‚’åˆ¤å®š
        if callable(loader_func):
            try:
                # é–¢æ•°ã®å ´åˆï¼šç›´æ¥å‘¼ã³å‡ºã—ã¦çµæœã‚’å–å¾—
                docs = loader_func(path)
                if isinstance(docs, list):
                    docs_all.extend(docs)
                else:
                    # ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                    docs = docs.load()
                    docs_all.extend(docs)
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {path}: {e}")
        else:
            # å¾“æ¥ã®æ–¹å¼
            loader = loader_func(path)
            docs = loader.load()
            docs_all.extend(docs)


def adjust_string(s):
    """
    Windowsç’°å¢ƒã§RAGãŒæ­£å¸¸å‹•ä½œã™ã‚‹ã‚ˆã†èª¿æ•´ï¼ˆå®Œå…¨UTF-8å¯¾å¿œç‰ˆï¼‰
    
    Args:
        s: èª¿æ•´ã‚’è¡Œã†æ–‡å­—åˆ—
    
    Returns:
        èª¿æ•´ã‚’è¡Œã£ãŸæ–‡å­—åˆ—
    """
    # èª¿æ•´å¯¾è±¡ã¯æ–‡å­—åˆ—ã®ã¿
    if type(s) is not str:
        return s

    # å…¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å…±é€šï¼šæœ€å°é™ã®å‡¦ç†ã®ã¿
    try:
        # Unicodeæ­£è¦åŒ–ã®ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¤‰æ›ã¯ä¸€åˆ‡è¡Œã‚ãªã„ï¼‰
        s = unicodedata.normalize('NFC', s)
        return s
    except Exception:
        # ä¾‹å¤–æ™‚ã¯ãã®ã¾ã¾è¿”ã™
        return s